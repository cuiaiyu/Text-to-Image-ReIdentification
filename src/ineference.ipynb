{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.wider_part_dataset import build_wider_dataloader\n",
    "from datasets.text_test_datasets import build_text_test_loader\n",
    "from datasets.image_test_datasets import build_image_test_loader\n",
    "from models.encoder import Model, MLP\n",
    "from evaluators.global_evaluator import GlobalEvaluator\n",
    "from evaluators.np_evaluator import NPEvaluator\n",
    "from loss.loss import crossmodal_triplet_loss, cos_distance, triplet_cos_loss\n",
    "from loggers.logger import Logger\n",
    "from manager import build_graph_optimizer\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "\n",
    "from attentions.rga_attention import RGA_attend_one_to_many_batch, RGA_attend_one_to_many\n",
    "import os\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from configs.args import load_arg_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug\n"
     ]
    }
   ],
   "source": [
    "parser = load_arg_parser()\n",
    "cfg = parser.parse_args(\"\")\n",
    "cfg.data_root = \"/shared/rsaas/aiyucui2/wider_person/\"\n",
    "root = cfg.data_root\n",
    "\n",
    "# data path\n",
    "cfg.anno_path = os.path.join(root, cfg.anno_path)\n",
    "cfg.img_dir = os.path.join(root, cfg.img_dir)\n",
    "cfg.val_anno_path = os.path.join(root, cfg.val_anno_path)\n",
    "cfg.val_img_dir = os.path.join(root, cfg.val_img_dir)\n",
    "cfg.gt_file_fn = os.path.join(root, cfg.gt_file_fn)\n",
    "\n",
    "# meta data path\n",
    "cfg.cheap_candidate_fn = os.path.join(root, cfg.cheap_candidate_fn)\n",
    "cfg.vocab_path = os.path.join(root, cfg.vocab_path)\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "# sys path\n",
    "cfg.model_path = os.path.join(root, cfg.model_path)\n",
    "cfg.output_path = os.path.join(root, cfg.output_path)\n",
    "ckpt_root = \"/shared/rsaas/aiyucui2/wider_person/checkpoints/reID/baseline\"\n",
    "load_exp_name = \"dist_fn_cosine_imgbb_resnet50_capbb_bigru_embed_size_1024_batch_96_lr_0.0002_step_size_10_captype_sent_img_meltlayer_6_np_True_sent_60_cap_10_6_both_fc_mute\"\n",
    "cfg.load_ckpt_fn = os.path.join(ckpt_root, load_exp_name, \"stage_2_id_match_epoch14.pt\")\n",
    "cfg.debug = False\n",
    "cfg.embed_size = 1024\n",
    "cfg.batch_size = 96\n",
    "cfg.img_backbone_opt = \"resnet50\"\n",
    "cfg.num_gpus = 1\n",
    "cfg.cap_backbone_opt = \"bigru\"\n",
    "cfg.dim = (384,128)\n",
    "cfg.dist_fn_opt = \"cosine\"\n",
    "cfg.np = True\n",
    "cfg.img_num_cut = 6\n",
    "cfg.img_num_cut = 1 if not cfg.np else cfg.img_num_cut\n",
    "cfg.sent_token_length = 60\n",
    "cfg.np_token_length = 6\n",
    "cfg.num_np_per_sent = 10\n",
    "\n",
    "\n",
    "\n",
    "cfg.cap_embed_type='sent'\n",
    "# exp_name\n",
    "cfg.exp_name = 'debug'\n",
    "cfg.model_path = os.path.join(\"/shared/rsaas/aiyucui2/wider_person\", cfg.model_path, cfg.exp_name)\n",
    "cfg.output_path = os.path.join(\"/shared/rsaas/aiyucui2/wider_person\", cfg.output_path, cfg.exp_name)\n",
    "\n",
    "if not os.path.exists(cfg.model_path):\n",
    "    os.mkdir(cfg.model_path)\n",
    "if not os.path.exists(cfg.output_path):\n",
    "    os.mkdir(cfg.output_path)\n",
    "# logger\n",
    "logger = Logger(\"test_np.txt\") #os.path.join(cfg.output_path, cfg.exp_name+\".txt\"))\n",
    "print(cfg.exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from evaluators.evaluator import *\n",
    "import torch\n",
    "import numpy as np\n",
    "# from attentions.rga_attention import RGA_attend_one_to_many_batch\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from loss.loss import cos_distance\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NPEvaluator(Evaluator):\n",
    "    def __init__(self, img_loader, cap_loader, gt_file_path,  embed_size, logger, dist_fn_opt, device):\n",
    "        super(NPEvaluator, self).__init__(img_loader, cap_loader, gt_file_path,  embed_size, logger)\n",
    "        self.device = device\n",
    "        # dist fn\n",
    "        self.dist_fn_opt = dist_fn_opt\n",
    "        if dist_fn_opt == 'euclidean':\n",
    "            self.dist = DistanceMetric.get_metric('euclidean').pairwise\n",
    "        else:\n",
    "            self.dist = cos_distance\n",
    "        \n",
    "    def populate_img_db(self):\n",
    "        K = self.embed_size\n",
    "        self.global_imgs = []\n",
    "        self.img_parts = []\n",
    "        self.encoder.eval(); self.mlp_img.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in tqdm(enumerate(self.img_loader),desc='build db global imgs'):\n",
    "                img, file_names = data\n",
    "                img_em, img_part = self.encoder(img.to(self.device))\n",
    "                self.img_parts.append(self.mlp_img(img_part))\n",
    "                self.global_imgs.append(img_em)\n",
    "        self.global_imgs = torch.cat(self.global_imgs)\n",
    "        self.img_parts = torch.cat(self.img_parts)\n",
    "        return self.global_imgs\n",
    "    \n",
    "    def populate_cap_db(self):\n",
    "        self.captions = {}\n",
    "        sent_parse = self.cap_loader.dataset.np_extractor.sent_parse\n",
    "        for ann in tqdm(self.cap_loader.dataset.anns, 'popluate cap db'):\n",
    "            caps = ann['captions']\n",
    "            self.captions[ann['file_path']] = sent_parse(caps[0])# + sent_parse(caps[1])\n",
    "    \n",
    "    def regional_alignment_image(self, caps, img_parts, dist_fn_opt):\n",
    "        scoremats = []\n",
    "        N, M, K = img_parts.size()\n",
    "        for cap in caps:\n",
    "            with torch.no_grad():\n",
    "                parts = RGA_attend_one_to_many_batch(cap[None], img_parts, dist_fn_opt)\n",
    "                if dist_fn_opt == \"cosine\":\n",
    "                    scores = 1 - F.cosine_similarity(cap[None], parts)\n",
    "                else:\n",
    "                    scores = F.pairwise_distance(cap[None], parts)\n",
    "                scoremats.append(scores.detach().cpu().numpy())\n",
    "        return np.array(scoremats)\n",
    "    \n",
    "    def regional_alignment_text(self, imgs, cap_parts, n2cs, dist_fn_opt):\n",
    "        scoremats = []\n",
    "        N, M, K = cap_parts.size()\n",
    "        for cap_part, n2c in zip(cap_parts, n2cs):\n",
    "            with torch.no_grad():\n",
    "                parts = RGA_attend_one_to_many_batch(imgs, cap_part[None,:n2c,:].expand(imgs.size(0), n2c, imgs.size(1)), dist_fn_opt)\n",
    "                if dist_fn_opt == \"cosine\":\n",
    "                    scores = 1 - F.cosine_similarity(imgs, parts)\n",
    "                else:\n",
    "                    scores = F.pairwise_distance(imgs, parts)\n",
    "                scoremats.append(scores.detach().cpu().numpy())\n",
    "        return np.array(scoremats)\n",
    "    \n",
    "   \n",
    "    def set_model(self, encoder, mlp_img, mlp_text):\n",
    "        self.encoder = encoder\n",
    "        self.mlp_img = mlp_img\n",
    "        self.mlp_text = mlp_text\n",
    "        self.populate_img_db()\n",
    "        self.populate_cap_db()\n",
    "        \n",
    "    def single_cap_encode(self, query):\n",
    "        self.encoder.eval()\n",
    "        self.mlp_text.eval()\n",
    "        cap_token, nps, num_nps = self.cap_loader.dataset._load_cap(query)\n",
    "        caps = torch.LongTensor(cap_token)[None].to(self.device)\n",
    "        nps = torch.LongTensor(nps)[None].to(self.device)\n",
    "        n2c = [num_nps]\n",
    "        N, M, T = nps.size()\n",
    "        self.global_caps = self.encoder(caps)\n",
    "        self.nps = self.encoder(nps.reshape(N*M, T).cuda())\n",
    "        self.n2cs = n2c\n",
    "        self.cap_parts = self.mlp_text(self.nps).reshape(N, M, -1)\n",
    "        \n",
    "    def inference(self, query, a=1, b=0, c=0, K=10):\n",
    "        '''\n",
    "        assume img_db has been populated already\n",
    "        '''\n",
    "        # encode query\n",
    "        self.single_cap_encode(query)\n",
    "        \n",
    "        # get scoremat\n",
    "        scoremat, scoremat2, scoremat3 = self.retrieval()\n",
    "        final_scoremat = a*scoremat + b*scoremat2 + c*scoremat3\n",
    "        \n",
    "        # return images\n",
    "        topk_images = np.argsort(final_scoremat[0, :])[:K]\n",
    "        imgs = [self.idx2img[i] for i in topk_images]\n",
    "        \n",
    "        return imgs\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def retrieval(self):\n",
    "        querys = self.global_caps.cpu().detach().numpy()\n",
    "        candidates = self.global_imgs.cpu().detach().numpy()\n",
    "        scoremat = self.dist(querys, candidates)\n",
    "        scoremat2 = self.regional_alignment_image(self.global_caps, self.img_parts, self.dist_fn_opt)\n",
    "        scoremat3 = self.regional_alignment_text(self.global_imgs, self.cap_parts, self.n2cs, self.dist_fn_opt)\n",
    "        return scoremat, scoremat2, scoremat3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": [
     9
    ]
   },
   "outputs": [],
   "source": [
    "# train loader\n",
    "# test loader (loading image and text separately)\n",
    "test_text_loader = build_text_test_loader(cfg) \n",
    "test_image_loader = build_image_test_loader(cfg) \n",
    "\n",
    "# Evaluator\n",
    "Evaluator = NPEvaluator if cfg.np else GlobalEvaluator\n",
    "evaluator = Evaluator(img_loader=test_image_loader, \n",
    "                          cap_loader=test_text_loader, \n",
    "                          gt_file_path=cfg.gt_file_fn,\n",
    "                          embed_size=cfg.embed_size,\n",
    "                          logger=logger,\n",
    "                          dist_fn_opt=\"cosine\",device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    ds = test_text_loader.dataset\n",
    "    extractor = ds.np_extractor\n",
    "    import nltk\n",
    "    import collections\n",
    "    num_nps_per_sent = collections.defaultdict(int)\n",
    "    num_nps = collections.defaultdict(int)\n",
    "    for cap in tqdm(ds.captions):\n",
    "        nps = extractor.sent_parse(cap)\n",
    "        num_nps_per_sent[len(nps)] += 1\n",
    "        for np in nps:\n",
    "            num_nps[len(np.split())] += 1\n",
    "\n",
    "    all_cnts = []\n",
    "    for num in num_nps_per_sent:\n",
    "        all_cnts += [num] * num_nps_per_sent[num]\n",
    "    all_cnts = sorted(all_cnts)\n",
    "    print(all_cnts[int(len(all_cnts)*0.98)])\n",
    "\n",
    "    all_cnts_np = []\n",
    "    for num in num_nps:\n",
    "        all_cnts_np += [num] * num_nps[num]\n",
    "    all_cnts_np = sorted(all_cnts_np)\n",
    "    print(all_cnts_np[int(len(all_cnts_np)*0.98)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trainer][init] load pre-trained model from /shared/rsaas/aiyucui2/wider_person/checkpoints/reID/baseline/dist_fn_cosine_imgbb_resnet50_capbb_bigru_embed_size_1024_batch_96_lr_0.0002_step_size_10_captype_sent_img_meltlayer_6_np_True_sent_60_cap_10_6_both_fc_mute/stage_2_id_match_epoch14.pt.\n",
      "[Trainer][init] load pre-trained id_cls from /shared/rsaas/aiyucui2/wider_person/checkpoints/reID/baseline/dist_fn_cosine_imgbb_resnet50_capbb_bigru_embed_size_1024_batch_96_lr_0.0002_step_size_10_captype_sent_img_meltlayer_6_np_True_sent_60_cap_10_6_both_fc_mute/stage_2_id_match_epoch14.pt.\n",
      "[Trainer][init] load pre-trained rga_img_mlp from /shared/rsaas/aiyucui2/wider_person/checkpoints/reID/baseline/dist_fn_cosine_imgbb_resnet50_capbb_bigru_embed_size_1024_batch_96_lr_0.0002_step_size_10_captype_sent_img_meltlayer_6_np_True_sent_60_cap_10_6_both_fc_mute/stage_2_id_match_epoch14.pt.\n",
      "[Trainer][init] load pre-trained rga_cap_mlp from /shared/rsaas/aiyucui2/wider_person/checkpoints/reID/baseline/dist_fn_cosine_imgbb_resnet50_capbb_bigru_embed_size_1024_batch_96_lr_0.0002_step_size_10_captype_sent_img_meltlayer_6_np_True_sent_60_cap_10_6_both_fc_mute/stage_2_id_match_epoch14.pt.\n",
      "[Trainer][init] model initialized.\n",
      "[Trainer][init] criterion initialized.\n"
     ]
    }
   ],
   "source": [
    "from manager import Manager, regional_alignment_text\n",
    "cfg.num_ids = 12003\n",
    "manager = Manager(cfg, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build db global imgs: 33it [00:05,  7.18it/s]\n",
      "popluate cap db: 100%|██████████| 3074/3074 [00:05<00:00, 582.99it/s]\n"
     ]
    }
   ],
   "source": [
    "evaluator.set_model(manager.model, manager.rga_img_mlp, manager.rga_cap_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       " <h2>query: A man holds a tote.</h2><table style=\"align:center\"><tr>\n",
       "<th> rank <th><th align=\"center\">No. 0</th><th align=\"center\">No. 1</th><th align=\"center\">No. 2</th><th align=\"center\">No. 3</th><th align=\"center\">No. 4</th><th align=\"center\">No. 5</th><th align=\"center\">No. 6</th><th align=\"center\">No. 7</th><th align=\"center\">No. 8</th><th align=\"center\">No. 9</th></tr>\n",
       "<tr>\n",
       "<td align=\"center\"> returned images <td><td align=\"right\" ><img src=\"../http/img/test_query/p1625_s11579.jpg\" height=128 width=48 /></td>\n",
       "<td align=\"right\" ><img src=\"../http/img/test_query/p7217_s9578.jpg\" height=128 width=48 /></td>\n",
       "<td align=\"right\" ><img src=\"../http/img/test_query/p7573_s10184.jpg\" height=128 width=48 /></td>\n",
       "<td align=\"right\" ><img src=\"../http/img/train_query/p11304_s12659.jpg\" height=128 width=48 /></td>\n",
       "<td align=\"right\" ><img src=\"../http/img/test_query/p1769_s1758.jpg\" height=128 width=48 /></td>\n",
       "<td align=\"right\" ><img src=\"../http/img/test_query/p1769_s1759.jpg\" height=128 width=48 /></td>\n",
       "<td align=\"right\" ><img src=\"../http/img/test_query/p805_s1487.jpg\" height=128 width=48 /></td>\n",
       "<td align=\"right\" ><img src=\"../http/img/train_query/p3450_s4401.jpg\" height=128 width=48 /></td>\n",
       "<td align=\"right\" ><img src=\"../http/img/train_query/p2358_s2765.jpg\" height=128 width=48 /></td>\n",
       "<td align=\"right\" ><img src=\"../http/img/train_query/p7916_s10254.jpg\" height=128 width=48 /></td>\n",
       "</tr>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " <h2>query: A man holds a tote.</h2><table style=\"align:center\"><tr>\n",
       "<th> rank <th><th align=\"center\">No. 0</th><th align=\"center\">No. 1</th><th align=\"center\">No. 2</th><th align=\"center\">No. 3</th><th align=\"center\">No. 4</th><th align=\"center\">No. 5</th><th align=\"center\">No. 6</th><th align=\"center\">No. 7</th><th align=\"center\">No. 8</th><th align=\"center\">No. 9</th></tr>\n",
       "<tr>\n",
       "<td align=\"center\"> returned images <td><td align=\"right\" ><img src=\"../http/img/CUHK01/0863004.png\" height=128 width=48 /></td>\n",
       "<td align=\"right\" ><img src=\"../http/img/test_query/p2473_s2984.jpg\" height=128 width=48 /></td>\n",
       "<td align=\"right\" ><img src=\"../http/img/test_query/p4095_s5146.jpg\" height=128 width=48 /></td>\n",
       "<td align=\"right\" ><img src=\"../http/img/test_query/p1769_s1758.jpg\" height=128 width=48 /></td>\n",
       "<td align=\"right\" ><img src=\"../http/img/test_query/p1769_s1759.jpg\" height=128 width=48 /></td>\n",
       "<td align=\"right\" ><img src=\"../http/img/CUHK03/1309_3.jpg\" height=128 width=48 /></td>\n",
       "<td align=\"right\" ><img src=\"../http/img/train_query/p6534_s8633.jpg\" height=128 width=48 /></td>\n",
       "<td align=\"right\" ><img src=\"../http/img/Market/1224_c5s3_014440_00.jpg\" height=128 width=48 /></td>\n",
       "<td align=\"right\" ><img src=\"../http/img/CUHK03/1004_4.jpg\" height=128 width=48 /></td>\n",
       "<td align=\"right\" ><img src=\"../http/img/test_query/p4095_s5145.jpg\" height=128 width=48 /></td>\n",
       "</tr>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "COLOR = ['white','yellow']\n",
    "query = 'A man holds a tote.'\n",
    "recalls = evaluator.inference(query,K=10)\n",
    "recalls_part = evaluator.inference(query, 0, 0.5, 0.5)\n",
    "def display_infer(query, recalls, caps, note='', img_root='../http/img'):\n",
    "    out_string = ' <h2>query: %s</h2>' % query\n",
    "    out_string += '<table style=\"align:center\">'\n",
    "    \n",
    "    out_string += '<tr>' + '\\n' \n",
    "    out_string += '<th> rank <th>'\n",
    "    for i, recall in enumerate(recalls):\n",
    "        out_string += '<th align=\"center\">No. %d</th>' % i\n",
    "    out_string += '</tr>' + '\\n'\n",
    "        \n",
    "    \n",
    "    out_string += '<tr>' + '\\n' \n",
    "    out_string += '<td align=\"center\"> %s <td>' % \"returned images\"\n",
    "    # recalls\n",
    "    for recall in recalls:\n",
    "        img_path = os.path.join(img_root, recall)\n",
    "        out_string += '<td align=\"right\" ><img src=\"%s\" height=128 width=48 /></td>' % img_path + '\\n'\n",
    "    out_string += '</tr>' + '\\n'\n",
    "    #out_string += '<td align=\"right\"> %s <td>' % \"associated noun phrases\"\n",
    "    for recall in []:#recalls:\n",
    "        out_string += '<td>'\n",
    "        for i,np in enumerate(caps[recall]):\n",
    "            color = COLOR[i%len(COLOR)]\n",
    "            out_string += \"<div style='background:%s;width=128'>%s</div>\" % (color,np)\n",
    "        out_string += '</td>'\n",
    "    out_string += '</tr>' + '\\n'\n",
    "    out_string += '</table>'\n",
    "    return HTML(out_string)\n",
    "    \n",
    "a = display_infer(query, recalls, evaluator.captions, 'global')\n",
    "b = display_infer(query, recalls_part, evaluator.captions, 'part')\n",
    "display(a)\n",
    "display(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Matching + ID Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
