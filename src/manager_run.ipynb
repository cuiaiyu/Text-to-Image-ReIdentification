{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.wider_part_dataset import build_wider_dataloader\n",
    "from datasets.text_test_datasets import build_text_test_loader\n",
    "from datasets.image_test_datasets import build_image_test_loader\n",
    "from models.encoder import Model, MLP\n",
    "from evaluators.global_evaluator import GlobalEvaluator\n",
    "from evaluators.np_evaluator import NPEvaluator\n",
    "from loss.loss import crossmodal_triplet_loss, cos_distance, triplet_cos_loss\n",
    "from loggers.logger import Logger\n",
    "from manager import build_graph_optimizer\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "\n",
    "from attentions.rga_attention import RGA_attend_one_to_many_batch, RGA_attend_one_to_many\n",
    "import os\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from configs.args import load_arg_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug\n"
     ]
    }
   ],
   "source": [
    "parser = load_arg_parser()\n",
    "cfg = parser.parse_args(\"\")\n",
    "cfg.data_root = \"/data/aiyucui2/wider\"\n",
    "root = cfg.data_root\n",
    "\n",
    "# data path\n",
    "cfg.anno_path = os.path.join(root, cfg.anno_path)\n",
    "cfg.img_dir = os.path.join(root, cfg.img_dir)\n",
    "cfg.val_anno_path = os.path.join(root, cfg.val_anno_path)\n",
    "cfg.val_img_dir = os.path.join(root, cfg.val_img_dir)\n",
    "cfg.gt_file_fn = os.path.join(root, cfg.gt_file_fn)\n",
    "\n",
    "# meta data path\n",
    "cfg.cheap_candidate_fn = os.path.join(root, cfg.cheap_candidate_fn)\n",
    "cfg.vocab_path = os.path.join(root, cfg.vocab_path)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "# sys path\n",
    "cfg.model_path = os.path.join(root, cfg.model_path)\n",
    "cfg.output_path = os.path.join(root, cfg.output_path)\n",
    "ckpt_root = \"/shared/rsaas/aiyucui2/wider_person/checkpoints/reID/baseline\"\n",
    "load_exp_name = \"dist_fn_cosine_imgbb_resnet50_capbb_bigru_embed_size_1024_batch_96_lr_0.0001_captype_sent_img_meltlayer_2_cos_margin_0.2_np_False\"\n",
    "cfg.load_ckpt_fn = os.path.join(ckpt_root, load_exp_name, \"stage_1_id_last.pt\")\n",
    "cfg.debug = False\n",
    "cfg.embed_size = 1024\n",
    "cfg.batch_size = 96\n",
    "cfg.img_backbone_opt = \"resnet50\"\n",
    "cfg.num_gpus = 1\n",
    "cfg.cap_backbone_opt = \"bigru\"\n",
    "cfg.dim = (384,128)\n",
    "cfg.dist_fn_opt = \"cosine\"\n",
    "cfg.np = True\n",
    "cfg.img_num_cut = 6\n",
    "cfg.img_num_cut = 1 if not cfg.np else cfg.img_num_cut\n",
    "cfg.sent_token_length = 60\n",
    "cfg.np_token_length = 6\n",
    "cfg.num_np_per_sent = 10\n",
    "\n",
    "\n",
    "\n",
    "cfg.cap_embed_type='sent'\n",
    "# exp_name\n",
    "cfg.exp_name = 'debug'\n",
    "cfg.model_path = os.path.join(\"/shared/rsaas/aiyucui2/wider_person\", cfg.model_path, cfg.exp_name)\n",
    "cfg.output_path = os.path.join(\"/shared/rsaas/aiyucui2/wider_person\", cfg.output_path, cfg.exp_name)\n",
    "\n",
    "if not os.path.exists(cfg.model_path):\n",
    "    os.mkdir(cfg.model_path)\n",
    "if not os.path.exists(cfg.output_path):\n",
    "    os.mkdir(cfg.output_path)\n",
    "# logger\n",
    "logger = Logger(\"test_np.txt\") #os.path.join(cfg.output_path, cfg.exp_name+\".txt\"))\n",
    "print(cfg.exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     9
    ]
   },
   "outputs": [],
   "source": [
    "from evaluators.evaluator import *\n",
    "import torch\n",
    "import numpy as np\n",
    "# from attentions.rga_attention import RGA_attend_one_to_many_batch\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from loss.loss import cos_distance\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NPEvaluator(Evaluator):\n",
    "    def __init__(self, img_loader, cap_loader, gt_file_path,  embed_size, logger, dist_fn_opt):\n",
    "        super(NPEvaluator, self).__init__(img_loader, cap_loader, gt_file_path,  embed_size, logger)\n",
    "        # dist fn\n",
    "        self.dist_fn_opt = dist_fn_opt\n",
    "        if dist_fn_opt == 'euclidean':\n",
    "            self.dist = DistanceMetric.get_metric('euclidean').pairwise\n",
    "        else:\n",
    "            self.dist = cos_distance\n",
    "        \n",
    "    def populate_img_db(self, encoder, img_mlp):\n",
    "        K = self.embed_size\n",
    "        self.global_imgs = []\n",
    "        self.img_parts = []\n",
    "        encoder.eval(); img_mlp.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in tqdm(enumerate(self.img_loader),desc='build db global imgs'):\n",
    "                img, file_names = data\n",
    "                img_em, img_part = encoder(img.cuda())\n",
    "                self.img_parts.append(img_mlp(img_part))\n",
    "                self.global_imgs.append(img_em)\n",
    "        self.global_imgs = torch.cat(self.global_imgs)\n",
    "        self.img_parts = torch.cat(self.img_parts)\n",
    "        return self.global_imgs\n",
    "    \n",
    "    def populate_cap_db(self, encoder, text_mlp):\n",
    "        K = self.embed_size\n",
    "        encoder.eval(); text_mlp.eval()\n",
    "        self.global_caps = []\n",
    "        self.cap_parts = []\n",
    "        self.n2cs = []\n",
    "        with torch.no_grad():\n",
    "            for i, batch in tqdm(enumerate(self.cap_loader),desc='build db global caps'):\n",
    "                caps, nps, n2c, file_names = batch\n",
    "                N, M, T = nps.size()\n",
    "                global_cap = encoder(caps.cuda())\n",
    "                nps = encoder(nps.reshape(N*M, T).cuda())\n",
    "                self.global_caps.append(global_cap)\n",
    "                self.n2cs.append(n2c)\n",
    "                self.cap_parts.append(text_mlp(nps).reshape(N, M, -1))\n",
    "        self.global_caps = torch.cat(self.global_caps)\n",
    "        self.cap_parts = torch.cat(self.cap_parts)\n",
    "        self.n2cs = torch.cat(self.n2cs)\n",
    "        return self.global_caps\n",
    "    \n",
    "    def regional_alignment_image(self, caps, img_parts, dist_fn_opt):\n",
    "        scoremats = []\n",
    "        N, M, K = img_parts.size()\n",
    "        for cap in tqdm(caps, \"scoremat_rga_img\"):\n",
    "            with torch.no_grad():\n",
    "                parts = RGA_attend_one_to_many_batch(cap[None], img_parts, dist_fn_opt)\n",
    "                if dist_fn_opt == \"cosine\":\n",
    "                    scores = 1 - F.cosine_similarity(cap[None], parts)\n",
    "                else:\n",
    "                    scores = F.pairwise_distance(cap[None], parts)\n",
    "                scoremats.append(scores.detach().cpu().numpy())\n",
    "        return np.array(scoremats)\n",
    "    \n",
    "    def regional_alignment_text(self, imgs, cap_parts, n2cs, dist_fn_opt):\n",
    "        scoremats = []\n",
    "        N, M, K = cap_parts.size()\n",
    "        for cap_part, n2c in tqdm(zip(cap_parts, n2cs), \"scoremat_rga_cap(nps)\"):\n",
    "            with torch.no_grad():\n",
    "                parts = RGA_attend_one_to_many_batch(imgs, cap_part[None,:n2c,:].expand(imgs.size(0), n2c, imgs.size(1)), dist_fn_opt)\n",
    "                if dist_fn_opt == \"cosine\":\n",
    "                    scores = 1 - F.cosine_similarity(imgs, parts)\n",
    "                else:\n",
    "                    scores = F.pairwise_distance(imgs, parts)\n",
    "                scoremats.append(scores.detach().cpu().numpy())\n",
    "        return np.array(scoremats)\n",
    "    \n",
    "    def evaluate(self, encoder, mlp_img, mlp_text, output_path=\"tmp.txt\"):\n",
    "        # compute global features\n",
    "        self.populate_img_db(encoder, mlp_img)\n",
    "        self.populate_cap_db(encoder, mlp_text)\n",
    "    \n",
    "        # global eval\n",
    "        # scoremat = self.retrieval()\n",
    "        scoremat_global, scoremat_img_rga, scoremat_cap_rga = self.retrieval()\n",
    "        acc = self.compute_acc(scoremat_global, output_path)\n",
    "        self.logger.log(\"[global] R@1: %.4f | R@5: %.4f | R@10: %.4f\" % (acc['top-1'], acc['top-5'], acc['top-10']))\n",
    "        acc = self.compute_acc(scoremat_img_rga, output_path)\n",
    "        self.logger.log(\"[img_rga] R@1: %.4f | R@5: %.4f | R@10: %.4f\" % (acc['top-1'], acc['top-5'], acc['top-10']))\n",
    "        acc = self.compute_acc(scoremat_cap_rga, output_path)\n",
    "        self.logger.log(\"[cap_rga] R@1: %.4f | R@5: %.4f | R@10: %.4f\" % (acc['top-1'], acc['top-5'], acc['top-10']))\n",
    "        \n",
    "        acc = self.compute_acc(scoremat_global + 0.5*scoremat_img_rga + 0.5*scoremat_cap_rga, output_path)\n",
    "        self.logger.log(\"[fusion] R@1: %.4f | R@5: %.4f | R@10: %.4f\" % (acc['top-1'], acc['top-5'], acc['top-10']))\n",
    "        return acc                         \n",
    "            \n",
    "    \n",
    "    def retrieval(self):\n",
    "        querys = self.global_caps.cpu().detach().numpy()\n",
    "        candidates = self.global_imgs.cpu().detach().numpy()\n",
    "        scoremat = self.dist(querys, candidates)\n",
    "        scoremat2 = self.regional_alignment_image(self.global_caps, self.img_parts, self.dist_fn_opt)\n",
    "        scoremat3 = self.regional_alignment_text(self.global_imgs, self.cap_parts, self.n2cs, self.dist_fn_opt)\n",
    "        return scoremat, scoremat2, scoremat3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     9
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ds] load annotations from /data/aiyucui2/wider/wider/train/train_anns_train.json\n",
      "size of dataset: 74264\n"
     ]
    }
   ],
   "source": [
    "# train loader\n",
    "train_loader = build_wider_dataloader(cfg)\n",
    "# test loader (loading image and text separately)\n",
    "test_text_loader = build_text_test_loader(cfg) \n",
    "test_image_loader = build_image_test_loader(cfg) \n",
    "\n",
    "# Evaluator\n",
    "Evaluator = NPEvaluator if cfg.np else GlobalEvaluator\n",
    "evaluator = Evaluator(img_loader=test_image_loader, \n",
    "                          cap_loader=test_text_loader, \n",
    "                          gt_file_path=cfg.gt_file_fn,\n",
    "                          embed_size=cfg.embed_size,\n",
    "                          logger=logger,\n",
    "                          dist_fn_opt=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    ds = test_text_loader.dataset\n",
    "    extractor = ds.np_extractor\n",
    "    import nltk\n",
    "    import collections\n",
    "    num_nps_per_sent = collections.defaultdict(int)\n",
    "    num_nps = collections.defaultdict(int)\n",
    "    for cap in tqdm(ds.captions):\n",
    "        nps = extractor.sent_parse(cap)\n",
    "        num_nps_per_sent[len(nps)] += 1\n",
    "        for np in nps:\n",
    "            num_nps[len(np.split())] += 1\n",
    "\n",
    "    all_cnts = []\n",
    "    for num in num_nps_per_sent:\n",
    "        all_cnts += [num] * num_nps_per_sent[num]\n",
    "    all_cnts = sorted(all_cnts)\n",
    "    print(all_cnts[int(len(all_cnts)*0.98)])\n",
    "\n",
    "    all_cnts_np = []\n",
    "    for num in num_nps:\n",
    "        all_cnts_np += [num] * num_nps[num]\n",
    "    all_cnts_np = sorted(all_cnts_np)\n",
    "    print(all_cnts_np[int(len(all_cnts_np)*0.98)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     15,
     83,
     86
    ]
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import os\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from models.encoder import Model, MLP\n",
    "from loss.loss import triplet_cos_loss, crossmodal_triplet_loss, triplet_cos_loss_inner\n",
    "\n",
    "from attentions.rga_attention import RGA_attend_one_to_many_batch, RGA_attend_one_to_many\n",
    "     \n",
    "from manager import Manager, regional_alignment_text\n",
    "\n",
    "\n",
    "def triplet_cos_loss_attention(fulls, parts, n2cs):\n",
    "    \"\"\"\n",
    "    fulls: N x E\n",
    "    parts: N x Z x E\n",
    "    pids: N\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    N, E = fulls.size()\n",
    "    Z = parts.size(1)\n",
    "    for i, (pid, n2c) in enumerate(zip(pids, n2cs)):\n",
    "        curr_part = parts[i:i+1].expand(N, Z, E)\n",
    "        curr_full = fulls[i:i+1]\n",
    "        cum_parts = RGA_attend_one_to_many_batch(fulls, curr_part, 'cosine')\n",
    "         # 1 x Z x E\n",
    "        neg_part = cum_parts[pids != pid]\n",
    "        pos_part = cum_parts[i:i+1].expand_as(neg_part)\n",
    "        \n",
    "        neg_full = fulls[pids != pid]\n",
    "        pos_full = fulls[i:i+1].expand_as(neg_full)\n",
    "        \n",
    "        loss = loss + triplet_cos_loss_inner(pos_part, pos_full, neg_full)\n",
    "        loss = loss + triplet_cos_loss_inner(pos_full, pos_part, neg_part)\n",
    "        \n",
    "    return loss / N\n",
    "\n",
    "def local_local_alignment_nps(parts, nps, n2cs):\n",
    "    attn_nps = []\n",
    "    for i in range(parts.size(1)):\n",
    "        curr_parts = parts[:, i, :]\n",
    "        curr_attn_nps = regional_alignment_text(curr_parts, nps, n2cs, 'cosine')\n",
    "        attn_nps.append(curr_attn_nps[None])\n",
    "    attn_nps = torch.cat(attn_nps).mean(0)\n",
    "    return attn_nps\n",
    "\n",
    "def local_local_alignment(parts, nps, n2cs):\n",
    "    B, N, E = parts.size()\n",
    "    B, M, E = nps.size()\n",
    "    \n",
    "    attned_parts = []\n",
    "    attned_nps = []\n",
    "    for i, n2c in zip(range(B), n2cs):\n",
    "        # for every caption vs. all \n",
    "        import pdb; pdb.set_trace()\n",
    "        curr_nps = nps[i, :n2c, :] # M x E\n",
    "        curr_parts = parts[i] # N x E\n",
    "        curr_nps = curr_nps / torch.norm(curr_nps, 1).expand(M, E)\n",
    "        curr_parts = curr_parts / torch.norm(curr_parts, 1).expand(N, E)\n",
    "        scoremat = torch.dot(curr_nps, curr_parts.T) # M x N\n",
    "        \n",
    "        coe_nps = F.softmax(scoremat, 1)[None].expand(M, N, E) # M x N \n",
    "        new_nps = nps[i:i+1, :n2c, :].expand_as(coe_nps) * coe_nps\n",
    "        attned_nps.append(new_nps.sum(1)[None])\n",
    "        coe_parts =  F.softmax(scoremat, 0)[None].expand_as(M, N, E) # N x E\n",
    "        new_parts = parts[i:i+1, :, :].expand_as(coe_nps) * coe_parts\n",
    "        attned_parts.append(new_parts.sum(0)[None]) \n",
    "    \n",
    "    attned_parts = torch.cat(attned_parts)\n",
    "    attned_nps = torch.cat(attned_nps)\n",
    "    return attned_parts, attned_nps\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "def regional_alignment_image(fulls, parts, dist_fn_opt):\n",
    "    return RGA_attend_one_to_many_batch(fulls, parts, dist_fn_opt)\n",
    "\n",
    "class AiyuManager(Manager):\n",
    "    def __init__(self, args, logger):\n",
    "        super(AiyuManager, self).__init__(args, logger)\n",
    "        \n",
    "    def _init_models(self):\n",
    "        # encoder\n",
    "        self.model = Model(embed_size=self.cfg.embed_size, \n",
    "                          image_opt=self.cfg.img_backbone_opt, \n",
    "                          caption_opt=self.cfg.cap_backbone_opt,\n",
    "                          cap_embed_type=self.cfg.cap_embed_type,\n",
    "                          img_num_cut=self.cfg.img_num_cut,\n",
    "                          regional_embed_size=self.cfg.regional_embed_size).cuda()\n",
    "        # id classifer\n",
    "        self.id_cls = nn.Linear(self.cfg.embed_size, self.cfg.num_ids).cuda()\n",
    "        # RGA image mlp\n",
    "        self.rga_img_mlp = MLP(self.cfg.regional_embed_size, self.cfg.embed_size).cuda()\n",
    "        # RGA text mlp\n",
    "        self.rga_cap_mlp = MLP(self.cfg.embed_size, self.cfg.embed_size).cuda()\n",
    "        # RGA image mlp\n",
    "        self.bfm_img_mlp = MLP(self.cfg.regional_embed_size, self.cfg.embed_size).cuda()\n",
    "        # RGA text mlp\n",
    "        self.bfm_cap_mlp = MLP(self.cfg.embed_size, self.cfg.embed_size).cuda()\n",
    "             \n",
    "        self.all_models = {\n",
    "            \"model\": self.model,\n",
    "            \"id_cls\": self.id_cls, \n",
    "            \"rga_img_mlp\": self.rga_img_mlp,\n",
    "            \"rga_cap_mlp\": self.rga_cap_mlp,\n",
    "            \"bfm_img_mlp\": self.bfm_img_mlp,\n",
    "            \"bfm_cap_mlp\": self.bfm_cap_mlp\n",
    "            \n",
    "        }\n",
    "         # load ckpt\n",
    "        self.reset_ckpt()\n",
    "        \n",
    "        # gpu\n",
    "        if self.cfg.num_gpus > 1:\n",
    "            for name in self.all_models.keys():\n",
    "                print('set data parallel to [%s]' % name)\n",
    "                self.all_models[name] = nn.DataParallel(self.all_models[name])\n",
    "           \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    def train_epoch_regional(self, train_data, optimizer, epoch, note=\"train\"):\n",
    "        self.model.train(); self.rga_img_mlp.train(); self.rga_cap_mlp.train(); \n",
    "\n",
    "        cum_tri_loss, cum_tri_image_regional_loss, cum_tri_text_regional_loss, cum_id_loss = 0.0, 0.0, 0.0, 0.0\n",
    "        for i, data in tqdm(enumerate(train_data), \"%s, epoch%d\" % (note,epoch)):\n",
    "            # load data\n",
    "            data = self.todevice(data)\n",
    "            (img, cap, nps, n2c, pid) = data\n",
    "\n",
    "\n",
    "            img, img_part = self.model(img)\n",
    "            cap = self.model(cap)\n",
    "            N, M, T = nps.size()\n",
    "            nps = self.model(nps.view(N*M,T)).view(N, M, cfg.embed_size)\n",
    "            #nps = self.rga_cap_mlp(self.np_encoder(nps))\n",
    "            \n",
    "            # part\n",
    "            nps = self.rga_cap_mlp(nps)\n",
    "            bfm_i =  self.bfm_img_mlp(img_part)\n",
    "            bfm_c =  self.bfm_cap_mlp(nps)\n",
    "            img_part = self.rga_img_mlp(img_part)\n",
    "\n",
    "            img_part1 = RGA_attend_one_to_many_batch(cap, img_part, self.cfg.dist_fn_opt)\n",
    "            cap_part1 = regional_alignment_text(img, nps, n2c, self.cfg.dist_fn_opt)\n",
    "            \n",
    "            bfm_i, bfm_c = local_local_alignment(img_part, nps, n2c)\n",
    "\n",
    "            # loss\n",
    "            tri_loss =  self.triplet_loss(img, cap, pid) \n",
    "            tri_text_regional_loss = self.triplet_loss(cap_part1, img, pid) #triplet_cos_loss_attention(img, nps, n2c, pid) ##s\n",
    "            tri_image_regional_loss = self.triplet_loss(img_part1, cap, pid) \n",
    "            id_loss = self.cls_loss(self.id_cls(img), pid) +  self.cls_loss(self.id_cls(cap), pid)\n",
    "            # id_loss = id_loss + self.cls_loss(self.id_cls(img_part), pid) +  self.cls_loss(self.id_cls(cap_part), pid)\n",
    "\n",
    "\n",
    "            loss = tri_loss + tri_image_regional_loss  + tri_text_regional_loss + id_loss\n",
    "\n",
    "            # backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            # log\n",
    "            cum_tri_loss += tri_loss.item()\n",
    "            cum_tri_image_regional_loss += tri_image_regional_loss.item()\n",
    "            cum_tri_text_regional_loss += tri_text_regional_loss.item()\n",
    "            cum_id_loss += id_loss.item()\n",
    "            \n",
    "            if (i+1) % self.cfg.print_freq == 0:\n",
    "                out_string = \"[ep-%d, bs-%d] \" % (epoch, i)\n",
    "                out_string += \"[id-loss] %.6f, \" % (cum_id_loss / self.cfg.print_freq)\n",
    "                out_string += \"[tri-loss] %.6f, \" % (cum_tri_loss / self.cfg.print_freq)\n",
    "                out_string += \"[img_rga] %.6f, \" %  (cum_tri_image_regional_loss / self.cfg.print_freq)\n",
    "                out_string += \"[cap_rga] %.6f \" % (cum_tri_text_regional_loss / self.cfg.print_freq)\n",
    "                self.log(out_string)\n",
    "                cum_tri_loss, cum_tri_image_regional_loss, cum_tri_text_regional_loss, cum_id_loss = 0.0, 0.0, 0.0, 0.0       \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trainer][init] load pre-trained model from /shared/rsaas/aiyucui2/wider_person/checkpoints/reID/baseline/dist_fn_cosine_imgbb_resnet50_capbb_bigru_embed_size_1024_batch_96_lr_0.0001_captype_sent_img_meltlayer_2_cos_margin_0.2_np_False/stage_1_id_last.pt.\n",
      "[Trainer][init] load pre-trained id_cls from /shared/rsaas/aiyucui2/wider_person/checkpoints/reID/baseline/dist_fn_cosine_imgbb_resnet50_capbb_bigru_embed_size_1024_batch_96_lr_0.0001_captype_sent_img_meltlayer_2_cos_margin_0.2_np_False/stage_1_id_last.pt.\n",
      "[Trainer][init] load pre-trained rga_img_mlp from /shared/rsaas/aiyucui2/wider_person/checkpoints/reID/baseline/dist_fn_cosine_imgbb_resnet50_capbb_bigru_embed_size_1024_batch_96_lr_0.0001_captype_sent_img_meltlayer_2_cos_margin_0.2_np_False/stage_1_id_last.pt.\n",
      "[Trainer][init] load pre-trained rga_cap_mlp from /shared/rsaas/aiyucui2/wider_person/checkpoints/reID/baseline/dist_fn_cosine_imgbb_resnet50_capbb_bigru_embed_size_1024_batch_96_lr_0.0001_captype_sent_img_meltlayer_2_cos_margin_0.2_np_False/stage_1_id_last.pt.\n",
      "[Trainer][init] criterion initialized.\n"
     ]
    }
   ],
   "source": [
    "cfg.num_ids = len(train_loader.dataset.person2label.values())\n",
    "manager = AiyuManager(cfg, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    if cfg.np:\n",
    "        acc = evaluator.evaluate(manager.model, manager.rga_img_mlp, manager.rga_cap_mlp)\n",
    "    else:\n",
    "        acc = evaluator.evaluate(manager.model)\n",
    "    logger.log('[cosine   ][global] R@1: %.4f | R@5: %.4f | R@10: %.4f' % (acc['top-1'], acc['top-5'], acc['top-10']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: ID Loss only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    logger.log(\"======== [Stage 1] ============\")\n",
    "    manager.melt_img_layer(num_layer_to_melt=1)\n",
    "    param_to_optimize = build_graph_optimizer([manager.model, manager.id_cls])\n",
    "    optimizer = optim.Adam(param_to_optimize, lr=1e-3)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10)\n",
    "    \n",
    "    for epoch in range(0):\n",
    "        manager.train_epoch_id(train_loader, optimizer, epoch, \"train-stage-1\")\n",
    "        acc = evaluator.evaluate(manager.model)\n",
    "        logger.log('[euclidean][global] R@1: %.4f | R@5: %.4f | R@10: %.4f' % (acc['top-1'], acc['top-5'], acc['top-10']))\n",
    "        acc = cos_evaluator.evaluate(manager.model)\n",
    "        logger.log('[cosine   ][global] R@1: %.4f | R@5: %.4f | R@10: %.4f' % (acc['top-1'], acc['top-5'], acc['top-10']))\n",
    "        scheduler.step()\n",
    "        manager.save_ckpt(epoch, acc, 'stage_1_id_last.pt')\n",
    "    manager.save_ckpt(epoch, acc, 'id_initialized.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Matching + ID Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "manager.melt_img_layer(num_layer_to_melt=2)\n",
    "if False:\n",
    "    if cfg.np:\n",
    "        param_to_optimize = build_graph_optimizer([manager.model, manager.id_cls, manager.rga_img_mlp, manager.rga_cap_mlp]) \n",
    "    else:\n",
    "        param_to_optimize = build_graph_optimizer([manager.model, manager.id_cls])  \n",
    "\n",
    "    optimizer = optim.Adam(param_to_optimize, lr=2e-4, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10)\n",
    "    train_epoch = manager.train_epoch_regional if cfg.np else manager.train_epoch_global\n",
    "    for epoch in range(15):\n",
    "        train_epoch(train_loader, optimizer, epoch, \"train-stage-2\")\n",
    "        if cfg.np:\n",
    "            cos_acc = evaluator.evaluate(manager.model, manager.rga_img_mlp, manager.rga_cap_mlp)\n",
    "        else:\n",
    "            cos_acc = evaluator.evaluate(manager.model)\n",
    "        logger.log('[cosine   ][global] R@1: %.4f | R@5: %.4f | R@10: %.4f' % (cos_acc['top-1'], cos_acc['top-5'], cos_acc['top-10']))\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ad8257c58d4987bde3795cae68ed2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='train-stage-3, epoch0', max=1, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (10) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9e8b519afc3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch_regional\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch_global\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train-stage-3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mcos_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrga_img_mlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrga_cap_mlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-473adb620ee6>\u001b[0m in \u001b[0;36mtrain_epoch_regional\u001b[0;34m(self, train_data, optimizer, epoch, note)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mcap_part1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregional_alignment_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn2c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist_fn_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mbfm_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbfm_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_local_alignment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_part\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn2c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;31m# loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-473adb620ee6>\u001b[0m in \u001b[0;36mlocal_local_alignment\u001b[0;34m(parts, nps, n2cs)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mcurr_nps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mn2c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# M x E\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mcurr_parts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# N x E\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mcurr_nps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_nps\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_nps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mcurr_parts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_parts\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_parts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mscoremat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_nps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_parts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# M x N\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (10) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "manager.melt_img_layer(num_layer_to_melt=0)\n",
    "if cfg.np:\n",
    "    param_to_optimize = build_graph_optimizer([manager.bfm_img_mlp, manager.bfm_cap_mlp]) \n",
    "\n",
    "    optimizer = optim.Adam(param_to_optimize, lr=2e-5, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10)\n",
    "    train_epoch = manager.train_epoch_regional if cfg.np else manager.train_epoch_global\n",
    "    for epoch in range(15):\n",
    "        train_epoch(train_loader, optimizer, epoch, \"train-stage-3\")\n",
    "        if cfg.np:\n",
    "            cos_acc = evaluator.evaluate(manager.model, manager.rga_img_mlp, manager.rga_cap_mlp)\n",
    "        else:\n",
    "            cos_acc = evaluator.evaluate(manager.model)\n",
    "        logger.log('[cosine   ][global] R@1: %.4f | R@5: %.4f | R@10: %.4f' % (cos_acc['top-1'], cos_acc['top-5'], cos_acc['top-10']))\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.np:\n",
    "    cos_acc = cos_evaluator.evaluate(manager.model, manager.rga_img_mlp, manager.rga_cap_mlp)\n",
    "else:\n",
    "    cos_acc = cos_evaluator.evaluate(manager.model)\n",
    "logger.log('[cosine   ][global] R@1: %.4f | R@5: %.4f | R@10: %.4f' % (cos_acc['top-1'], cos_acc['top-5'], cos_acc['top-10']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
