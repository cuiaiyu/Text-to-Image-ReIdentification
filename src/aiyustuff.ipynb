{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active FULL Training Code (Basic - Global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.wider_global_dataset import build_wider_dataloader\n",
    "from datasets.text_test_datasets import build_text_test_loader\n",
    "from datasets.image_test_datasets import build_image_test_loader\n",
    "from models.encoder import Model, MLP\n",
    "from evaluators.global_evaluator import GlobalEvaluator\n",
    "from loss.loss import crossmodal_triplet_loss, cos_distance\n",
    "from loggers.logger import Logger\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "import os\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from configs.args import load_arg_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dist_fn_cosine_imgbb_resnet18_capbb_bigru_embed_size_512_batch_96_lr_0.0001_captype_sent\n"
     ]
    }
   ],
   "source": [
    "parser = load_arg_parser()\n",
    "cfg = parser.parse_args(\"\")\n",
    "cfg.data_root = \"/data/aiyucui2/wider\"\n",
    "root = cfg.data_root\n",
    "\n",
    "# data path\n",
    "cfg.anno_path = os.path.join(root, cfg.anno_path)\n",
    "cfg.img_dir = os.path.join(root, cfg.img_dir)\n",
    "cfg.val_anno_path = os.path.join(root, cfg.val_anno_path)\n",
    "cfg.val_img_dir = os.path.join(root, cfg.val_img_dir)\n",
    "cfg.gt_file_fn = os.path.join(root, cfg.gt_file_fn)\n",
    "\n",
    "# meta data path\n",
    "cfg.cheap_candidate_fn = os.path.join(root, cfg.cheap_candidate_fn)\n",
    "cfg.vocab_path = os.path.join(root, cfg.vocab_path)\n",
    "\n",
    "# sys path\n",
    "cfg.model_path = os.path.join(root, cfg.model_path)\n",
    "cfg.output_path = os.path.join(root, cfg.output_path)\n",
    "ckpt_root = \"/shared/rsaas/aiyucui2/wider_person/checkpoints/reID\"\n",
    "load_exp_name = \"dist_fn_cosine_imgbb_resnet18_capbb_bigru_embed_size_512_batch_96_lr_0.0001_captype_sent_img_meltlayer_8\"\n",
    "cfg.load_ckpt_fn = '0' #os.path.join(ckpt_root, load_exp_name, \"stage1.pt\")\n",
    "cfg.debug = False\n",
    "cfg.embed_size = 512\n",
    "cfg.batch_size = 96\n",
    "cfg.img_backbone_opt = \"resnet18\"\n",
    "cfg.cap_backbone_opt = \"bigru\"\n",
    "cfg.dim = (384,128)\n",
    "cfg.dist_fn_opt = \"cosine\"\n",
    "cfg.np = False\n",
    "cfg.img_num_cut = 6\n",
    "cfg.img_num_cut = 1 if not cfg.np else cfg.img_num_cut\n",
    "cfg.cap_embed_type='sent'\n",
    "# exp_name\n",
    "exp_name = \"dist_fn_{}_imgbb_{}_capbb_{}_embed_size_{}_batch_{}_lr_{}_captype_{}\".format(cfg.dist_fn_opt,\n",
    "                                                                       cfg.img_backbone_opt,\n",
    "                                                                       cfg.cap_backbone_opt,\n",
    "                                                                       cfg.embed_size,\n",
    "                                                                       cfg.batch_size,\n",
    "                                                                       cfg.lr,\n",
    "                                                                                         cfg.cap_embed_type)\n",
    "# logger\n",
    "logger = Logger(\"test.txt\") #os.path.join(cfg.output_path, cfg.exp_name+\".txt\"))\n",
    "print(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(anno_path='/data/aiyucui2/wider/wider/train/train_anns_train.json', batch_size=96, cap_backbone_opt='bigru', cap_embed_type='sent', cheap_candidate_fn='/data/aiyucui2/wider/wider_graph/summer_best_val1_top100.pkl', cheap_eval=True, ckpt_freq=10, cos_margin=0.5, data_root='/data/aiyucui2/wider', debug=False, dim=(384, 128), dist_fn_opt='cosine', embed_size=512, experiment_name='default', gt_file_fn='/data/aiyucui2/wider/wider/val1/val_label.json', image_melt_layer=1, img_backbone_opt='resnet18', img_dir='/data/aiyucui2/wider/wider/train/img', img_num_cut=1, load_ckpt_fn='0', load_model_path='starter_bert_resnet50_2048.pt', lr=0.0001, mode='train', model_path='/data/aiyucui2/wider/checkpoints/reID/', momentum=0.9, np=False, num_epochs=25, num_epochs_stage1=20, num_epochs_stage2=60, num_gpu=1, num_gpus=1, num_workers=8, optimizer='Adam', output_path='/data/aiyucui2/wider/outputs/reID/', print_freq=50, regional_embed_size=256, step_size=15, text_melt_layer=0, token_length=40, val_anno_path='/data/aiyucui2/wider/wider/val1/val1_anns.json', val_img_dir='/data/aiyucui2/wider/wider/val1/img', vocab_path='/data/aiyucui2/wider/wider_graph/raw_vocab_th20.json', weight_decay=0.0005)\n"
     ]
    }
   ],
   "source": [
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ds] load annotations from /data/aiyucui2/wider/wider/train/train_anns_train.json\n",
      "size of dataset: 74264\n"
     ]
    }
   ],
   "source": [
    "# train loader\n",
    "train_loader = build_wider_dataloader(cfg)\n",
    "\n",
    "# test loader (loading image and text separately)\n",
    "test_text_loader = build_text_test_loader(cfg) \n",
    "test_image_loader = build_image_test_loader(cfg) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12003\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader.dataset.person2label.values()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([  9,  14,   6,   8,   2,  24,   4,  10,  97, 144,  58,   4,  59,  17,\n",
      "          3,  19,  16,  50, 494, 123,  30, 141,   3,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1]), 'train_query/p8848_s17661.jpg')\n",
      "tensor([ 51,  21,  16,  20,  39,  80,  23,   7,   2,  10, 289,  38,   7,  31,\n",
      "          2,  35,  69,  42,  27,  54,  39,   7,   4,  24,  36,   4, 176,  10,\n",
      "         13,   3,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "# functions to show an image\n",
    "print(test_text_loader.dataset[1])\n",
    "print(train_loader.dataset[1][5])\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "dataiter = iter(test_image_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "\n",
    "from models.encoder import Model, MLP               \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(embed_size=cfg.embed_size, \n",
    "              image_opt=cfg.img_backbone_opt, \n",
    "              caption_opt=cfg.cap_backbone_opt,\n",
    "              cap_embed_type=cfg.cap_embed_type,\n",
    "              img_num_cut=cfg.img_num_cut,\n",
    "              regional_embed_size=cfg.regional_embed_size).cuda()\n",
    "\n",
    "if cfg.load_ckpt_fn != \"0\":\n",
    "    logger.log(\"[Model] load pre-trained model from %s.\" % cfg.load_ckpt_fn)\n",
    "    ckpt = torch.load(cfg.load_ckpt_fn)\n",
    "    model.load_state_dict(ckpt[\"model\"], False)\n",
    "    \n",
    "id_cls = nn.Sequential(\n",
    "    nn.Linear(cfg.embed_size, 164),\n",
    "    nn.Softmax()\n",
    ").cuda()\n",
    "img_mlp = MLP(cfg.regional_embed_size, cfg.embed_size).cuda()\n",
    "cap_mlp = MLP(cfg.embed_size, cfg.embed_size).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "def triplet_cos_loss(x, pos, neg, margin=0.2):\n",
    "    def cos_dist(x,y):\n",
    "        # import pdb; pdb.set_trace()\n",
    "        return 1 - torch.sum(x*y, 1) / (torch.norm(x, dim=1)*torch.norm(y, dim=1))\n",
    "    pos_dist = cos_dist(x, pos)\n",
    "    neg_dist = cos_dist(x, neg)\n",
    "    scores = torch.clamp(pos_dist - neg_dist + margin, min=0)\n",
    "    return scores.mean()\n",
    "    \n",
    "\n",
    "if cfg.dist_fn_opt == \"euclidean\":\n",
    "    dist_fn = DistanceMetric.get_metric('euclidean').pairwise\n",
    "    triplet_loss = nn.TripletMarginLoss()\n",
    "elif cfg.dist_fn_opt == \"cosine\":\n",
    "    dist_fn = cos_distance\n",
    "    triplet_loss = triplet_cos_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Misc Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from evaluators.evaluator import Evaluator\n",
    "from evaluators.global_evaluator import GlobalEvaluator\n",
    "from evaluators.np_evaluator import NPEvaluator\n",
    "from attentions.rga_attention import RGA_attend_one_to_many_batch, RGA_attend_one_to_many\n",
    "# from tqdm import tqdm\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "if cfg.np:\n",
    "    Evaluator = NPEvaluator\n",
    "else:\n",
    "    Evaluator = GlobalEvaluator\n",
    "\n",
    "evaluator = Evaluator(img_loader=test_image_loader, \n",
    "                          cap_loader=test_text_loader, \n",
    "                          gt_file_path=cfg.gt_file_fn,\n",
    "                          embed_size=cfg.embed_size,\n",
    "                          logger=logger,\n",
    "                          dist_fn_opt=\"euclidean\")\n",
    "cos_evaluator = Evaluator(img_loader=test_image_loader, \n",
    "                          cap_loader=test_text_loader, \n",
    "                          gt_file_path=cfg.gt_file_fn,\n",
    "                          embed_size=cfg.embed_size,\n",
    "                          logger=logger,\n",
    "                          dist_fn_opt=\"cosine\")\n",
    "\n",
    "\n",
    "def build_graph_optimizer(models):\n",
    "    if not isinstance(models, list):\n",
    "        models = [models]\n",
    "    params_to_optimize = []\n",
    "    for model in models:\n",
    "        if model and hasattr(model, '_parameters'):\n",
    "            for param in model.parameters():\n",
    "                if param.requires_grad == True:\n",
    "                    params_to_optimize.append(param)\n",
    "    return params_to_optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'evaluators.global_evaluator.GlobalEvaluator'>\n"
     ]
    }
   ],
   "source": [
    "print(type(cos_evaluator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     11,
     21
    ]
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import torchnet as tnt\n",
    "import os\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from models.model import build_dual_encoder\n",
    "from loss.loss import triplet_cos_loss, crossmodal_triplet_loss\n",
    "\n",
    "def regional_alignment_text(fulls, parts, p2fs, dist_fn_opt):\n",
    "    start_index = 0\n",
    "    aligned = []\n",
    "    for i, jump in enumerate(p2fs):\n",
    "        curr_parts = parts[start_index:start_index + jump]\n",
    "        start_index += jump\n",
    "        curr_full = fulls[i:i+1]\n",
    "        aligned.append(RGA_attend_one_to_many(curr_full, curr_parts, dist_fn_opt))\n",
    "    return torch.cat(aligned)\n",
    "\n",
    "def regional_alignment_image(fulls, parts, dist_fn_opt):\n",
    "    return RGA_attend_one_to_many_batch(fulls, parts, dist_fn_opt)\n",
    "  \n",
    "    \n",
    "class Manager:\n",
    "    def __init__(self, args, logger):\n",
    "        self.cfg = args\n",
    "        self._init_models()\n",
    "        self._init_criterion()\n",
    "        self.log = logger.log\n",
    "    \n",
    "    def _init_criterion(self):\n",
    "        if self.cfg.dist_fn_opt == \"cosine\":\n",
    "            self.triplet_loss = triplet_cos_loss\n",
    "        elif self.cfg.dist_fn_opt == \"euclidean\":\n",
    "            self.triplet_loss = nn.TripletMarginLoss()\n",
    "        self.cls_loss = nn.CrossEntropyLoss()\n",
    "        self.log(\"[Trainer][init] criterion initialized.\")\n",
    "\n",
    "    def _init_models(self):\n",
    "        self.model = Model(embed_size=self.cfg.embed_size, \n",
    "                          image_opt=self.cfg.img_backbone_opt, \n",
    "                          caption_opt=self.cfg.cap_backbone_opt,\n",
    "                          cap_embed_type=self.cfg.cap_embed_type,\n",
    "                          img_num_cut=self.cfg.img_num_cut,\n",
    "                          regional_embed_size=self.cfg.regional_embed_size).cuda()\n",
    "        self.id_cls = nn.Linear(cfg.embed_size, cfg.num_ids)\n",
    "        self.rga_img_mlp = MLP(self.cfg.regional_embed_size, self.cfg.embed_size).cuda()\n",
    "        self.rga_cap_mlp = MLP(self.cfg.embed_size, self.cfg.embed_size).cuda()\n",
    "        \n",
    "        # load ckpt\n",
    "        self.reset_ckpt()\n",
    "        \n",
    "        # gpu\n",
    "        self.all_models = {\n",
    "            \"model\": self.model,\n",
    "            \"id_cls\": self.id_cls, \n",
    "            \"rga_img_mlp\": self.rga_img_mlp,\n",
    "            \"rga_cap_mlp\": self.rga_cap_mlp,\n",
    "        }\n",
    "        self.log(\"[Trainer][init] model initialized.\")\n",
    "\n",
    "    def reset_ckpt(self):\n",
    "        self.start_epoch = 0\n",
    "        self.acc_history = []\n",
    "        self.best_acc = (0, self.start_epoch)\n",
    "        if cfg.load_ckpt_fn == \"0\":\n",
    "            self.log(\"[Trainer][init] initialize fresh model.\")\n",
    "            return\n",
    "        ckpt = torch.load(cfg.load_ckpt_fn)\n",
    "        self.start_epoch = ckpt[\"epoch\"] + 1\n",
    "        self.acc_history = ckpt[\"acc_history\"]\n",
    "        for name, network in self.all_models.items():\n",
    "            if name in ckpt:\n",
    "                network.load_state_dict(ckpt[name], False)\n",
    "                self.log(\"[Trainer][init] load pre-trained %s from %s.\" % (network, cfg.load_ckpt_fn))\n",
    "\n",
    "              \n",
    "    def save_ckpt(self, epoch, acc, fn):\n",
    "        # update acc history \n",
    "        self.acc_history.append((acc, epoch))\n",
    "        if acc > self.best_acc[0]:\n",
    "            self.best_acc = (acc, epoch)\n",
    "        # ckpt \n",
    "        ckpt = {\n",
    "            \"epoch\": epoch,\n",
    "            \"acc_history\": self.acc_history,\n",
    "            \"best_acc\": self.best_acc,\n",
    "            }\n",
    "        for name, network in self.all_models.items():\n",
    "            ckpt[name] = network.module.state_dict() if isinstance(network, nn.DataParallel) else network.state_dict(),\n",
    "\n",
    "        path = os.path.join(self.cfg.model_path, fn)\n",
    "        torch.save(ckpt, path)\n",
    "            \n",
    "    def todevice(self, batch):\n",
    "        ret = []\n",
    "        for arg in batch:\n",
    "            if isinstance(arg, torch.Tensor):\n",
    "                arg = arg.cuda()\n",
    "            ret.append(arg)\n",
    "        return tuple(ret)\n",
    "    \n",
    "    def melt_img_layer(self, num_layer_to_melt=1):\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            self.model.module.img_backbone.melt_layer(8 - num_layer_to_melt)\n",
    "        else:\n",
    "            self.model.img_backbone.melt_layer(8 - num_layer_to_melt)\n",
    "     \n",
    "    def train_epoch_global(train_data, optimizer, epoch, note=\"train\"):\n",
    "        self.model.train()\n",
    "        cum_tri_loss, cum_id_loss = 0.0, 0.0\n",
    "        for i, data in tqdm(enumerate(train_data), \"%s, epoch%d\" % (note, epoch)):\n",
    "            # load data\n",
    "            data = self.todevice(data)\n",
    "            (img,pos_img,neg_img, cap, pos_cap, neg_cap, pid, pos_pid, neg_pid) = data\n",
    "            \n",
    "            # encode\n",
    "            img, pos_img, neg_img = self.model(img), self.model(pos_img), self.model(neg_img)\n",
    "            cap, pos_cap, neg_cap = self.model(cap), self.model(pos_cap), self.model(neg_cap)\n",
    "\n",
    "            # loss\n",
    "            tri_loss =  crossmodal_triplet_loss(img,pos_img,neg_img, \n",
    "                                                  cap, pos_cap, neg_cap, \n",
    "                                                  self.triplet_loss, self.cfg.dist_fn_opt)  \n",
    "            id_loss = self.cls_loss(self.id_cls(img), pid) +  self.cls_loss(self.id_cls(cap), pid)\n",
    "            loss = tri_loss + id_loss\n",
    "\n",
    "            # backpropagation\n",
    "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "            # log\n",
    "            cum_tri_loss += tri_loss.item()\n",
    "            cum_id_loss += id_loss.item()\n",
    "            if (i+1) % self.cfg.print_freq == 0:\n",
    "                out_string = \"[ep-%d, bs-%d] \" % (epoch, i)\n",
    "                out_string += \"[tri-loss] %.6f, \" % cum_tri_loss / self.cfg.print_freq\n",
    "                out_string += \"[id-loss] %.6f, \" % (cum_id_loss / self.cfg.print_freq)\n",
    "                self.log(out_string)\n",
    "                cum_tri_loss, cum_id_loss = 0.0, 0.0\n",
    "                \n",
    "    def train_epoch_regional(train_data, optimizer, epoch, note=\"train\"):\n",
    "        self.model.train(); self.rga_img_mlp.train(); self.rga_cap_mlp.train()\n",
    "\n",
    "        cum_tri_loss, cum_tri_image_regional_loss, cum_tri_text_regional_loss, cum_id_loss = 0.0, 0.0, 0.0, 0.0\n",
    "        for i, data in tqdm(enumerate(train_data), \"%s, epoch%d\" % (note,epoch)):\n",
    "            # load data\n",
    "            data = self.todevice(data)\n",
    "            (img, pos_img, neg_img, \n",
    "             cap, pos_cap, neg_cap,\n",
    "             nps, pos_nps, neg_nps,\n",
    "             n2c, pos_n2c, neg_n2c,\n",
    "             pid, pos_pid, neg_pid) = data\n",
    "\n",
    "\n",
    "            img, img_part = self.model(img)\n",
    "            pos_img, pos_img_part = self.model(pos_img)\n",
    "            neg_img, neg_img_part = self.model(neg_img)\n",
    "            cap, pos_cap, neg_cap = self.model(cap), self.model(pos_cap), self.model(neg_cap)\n",
    "            \n",
    "            N, M, T = nps.size()\n",
    "            nps = self.rga_cap_mlp(self.model(nps.reshape(-1, T))).reshape(N, M, -1)\n",
    "            pos_nps = self.rga_cap_mlp(self.model(pos_nps.reshape(-1, T))).reshape(N, M, -1)\n",
    "            neg_nps = self.rga_cap_mlp(self.model(neg_nps.reshape(-1, T))).reshape(N, M, -1)\n",
    "            \n",
    "            # part\n",
    "            img_part = self.rga_img_mlp(img_part)\n",
    "            pos_img_part = self.rga_img_mlp(pos_img_part)\n",
    "            neg_img_part = self.rga_img_mlp(neg_img_part)\n",
    "\n",
    "            img_part = RGA_attend_one_to_many_batch(cap, img_part, self.cfg.dist_fn_opt)\n",
    "            pos_img_part = RGA_attend_one_to_many_batch(pos_cap, pos_img_part, self.cfg.dist_fn_opt)\n",
    "            neg_img_part = RGA_attend_one_to_many_batch(neg_cap, neg_img_part, self.cfg.dist_fn_opt)\n",
    "            #cap_part = regional_alignment_text(img, nps, n2c, cfg.dist_fn_opt)\n",
    "            #pos_cap_part = regional_alignment_text(pos_img, pos_nps, pos_n2c, cfg.dist_fn_opt)\n",
    "            #neg_cap_part = regional_alignment_text(neg_img, neg_nps, neg_n2c, cfg.dist_fn_opt)\n",
    "            cap_part = RGA_attend_one_to_many_batch(img, nps, self.cfg.dist_fn_opt)\n",
    "            pos_cap_part = RGA_attend_one_to_many_batch(pos_img, pos_nps, self.cfg.dist_fn_opt)\n",
    "            neg_cap_part = RGA_attend_one_to_many_batch(neg_img, neg_nps, self.cfg.dist_fn_opt)\n",
    "\n",
    "            # loss\n",
    "            tri_loss =  crossmodal_triplet_loss(img,pos_img,neg_img, \n",
    "                                                  cap, pos_cap, neg_cap, \n",
    "                                                  triplet_loss, self.cfg.dist_fn_opt) \n",
    "            tri_image_regional_loss =  crossmodal_triplet_loss(img_part,pos_img_part,neg_img_part, \n",
    "                                                  cap, pos_cap, neg_cap, \n",
    "                                                  triplet_loss, self.cfg.dist_fn_opt) \n",
    "            tri_text_regional_loss =  crossmodal_triplet_loss(img,pos_img,neg_img, \n",
    "                                                  cap_part, pos_cap_part, neg_cap_part, \n",
    "                                                  triplet_loss, self.cfg.dist_fn_opt) \n",
    "            id_loss = self.cls_loss(self.id_cls(img), pid) +  self.cls_loss(self.id_cls(cap), pid)\n",
    "\n",
    "\n",
    "            loss = tri_loss + tri_image_regional_loss  + tri_text_regional_loss + id_loss\n",
    "\n",
    "            # backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            # log\n",
    "            cum_tri_loss += tri_loss.item()\n",
    "            cum_tri_image_regional_loss += tri_image_regional_loss.item()\n",
    "            cum_tri_text_regional_loss += tri_text_regional_loss.item()\n",
    "            cum_id_loss += id_loss.item()\n",
    "            \n",
    "            if (i+1) % self.cfg.print_freq == 0:\n",
    "                out_string = \"[ep-%d, bs-%d] \" % (epoch, i)\n",
    "                out_string += \"[id-loss] %.6f, \" % (cum_id_loss / self.cfg.print_freq)\n",
    "                out_string += \"[tri-loss] %.6f, \" % cum_tri_loss / self.cfg.print_freq\n",
    "                out_string += \"[img_rga] %.6f, \" %  cum_tri_image_regional_loss / self.cfg.print_freq\n",
    "                out_string += \"[cap_rga] %.6f \" % cum_tri_text_regional_loss / self.cfg.print_freq\n",
    "                self.log(out_string)\n",
    "                cum_tri_loss, cum_tri_image_regional_loss, cum_tri_text_regional_loss, cum_id_loss = 0.0, 0.0, 0.0, 0.0\n",
    "               \n",
    "            \n",
    "            \n",
    "    def train_epoch_id(train_data, optimizer, epoch, note=\"train\"):\n",
    "        model.train()\n",
    "        cum_loss = 0.0\n",
    "        for i, data in tqdm(enumerate(train_data), \"%s, epoch%d\" % (note,epoch)):\n",
    "            # load data\n",
    "            data = self.todevice(data)\n",
    "            (img,pos_img,neg_img, cap, pos_cap, neg_cap, pid, pos_pid, neg_pid) = data\n",
    "            img = model(img)\n",
    "            cap = model(cap)\n",
    "\n",
    "            # loss\n",
    "            loss = 0.0\n",
    "            loss = loss + self.cls_loss(self.id_cls(img), pid) +  self.cls_loss(self.id_cls(cap), pid)\n",
    "\n",
    "            # backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cum_loss += loss.item()\n",
    "\n",
    "            # log\n",
    "            if (i+1) % self.cfg.print_freq == 0:\n",
    "                print(\"ep-%d, bs-%d, [id-loss] %.6f\" % (epoch, i, cum_loss / self.cfg.print_freq))\n",
    "                cum_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Id Loss initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d41ee4250054343b933c8f8d940313e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='train-stage-1, epoch0', max=1, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_EXECUTION_FAILED",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-fadadda3d3ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch_stage1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train-stage-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[euclidean][global] R@1: %.4f | R@5: %.4f | R@10: %.4f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'top-1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'top-5'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'top-10'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-fadadda3d3ed>\u001b[0m in \u001b[0;36mtrain_epoch_stage1\u001b[0;34m(train_data, model, classifier, optimizer, cls_loss, note)\u001b[0m\n\u001b[1;32m      9\u001b[0m          pid, pos_pid, neg_pid) = data\n\u001b[1;32m     10\u001b[0m         \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mcap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_cap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_cap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_cap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_cap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/maskrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cross_ReID/src/models/encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_backbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcap_backbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/maskrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cross_ReID/src/models/texts/gru_backbone.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, caps)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcaps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcap_embed_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"word\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/maskrcnn/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/maskrcnn/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             result = _impl(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n\u001b[0;32m--> 209\u001b[0;31m                            self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED"
     ]
    }
   ],
   "source": [
    "def train_epoch_stage1(train_data, model, classifier, optimizer, cls_loss, note=\"train\"):\n",
    "    model.train()\n",
    "    classifier.train()\n",
    "    cum_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(train_data), \"%s, epoch%d\" % (note,epoch)):\n",
    "        # load data\n",
    "        (img, pos_img, neg_img, \n",
    "         cap, pos_cap, neg_cap, \n",
    "         pid, pos_pid, neg_pid) = data\n",
    "        img, pos_img, neg_img = model(img.cuda()), model(pos_img.cuda()), model(neg_img.cuda())\n",
    "        cap, pos_cap, neg_cap = model(cap.cuda()), model(pos_cap.cuda()), model(neg_cap.cuda())\n",
    "        \n",
    "        # loss\n",
    "        loss = 0.0\n",
    "        loss = loss + cls_loss(classifier(img), pid.cuda()) +  cls_loss(classifier(cap), pid.cuda())\n",
    "        # import pdb; pdb.set_trace()\n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cum_loss += loss.item()\n",
    "        \n",
    "        # log\n",
    "        if (i+1) % 64 == 0:\n",
    "            print(\"batch %d, loss %.6f\" % (i, cum_loss/64))\n",
    "            cum_loss = 0.0\n",
    "    return model\n",
    "\n",
    "\n",
    "if True:\n",
    "    num_ids = len(train_loader.dataset.person2label.values())\n",
    "    classifier = nn.Sequential(\n",
    "        nn.Linear(cfg.embed_size, num_ids),\n",
    "        # nn.Softmax()\n",
    "    )\n",
    "    classifier.cuda()\n",
    "    # stage 1 - image channel forzen\n",
    "    cls_loss = nn.CrossEntropyLoss()\n",
    "    model.img_backbone.melt_layer(0)\n",
    "    param_to_optimize = build_graph_optimizer([model, classifier])\n",
    "    optimizer = optim.Adam(param_to_optimize, lr=2e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10)\n",
    "    for epoch in range(15):\n",
    "        model = train_epoch_stage1(train_loader, model, classifier, optimizer, cls_loss, \"train-stage-1\")\n",
    "        acc = evaluator.evaluate(model)\n",
    "        logger.log('[euclidean][global] R@1: %.4f | R@5: %.4f | R@10: %.4f' % (acc['top-1'], acc['top-5'], acc['top-10']))\n",
    "        acc = cos_evaluator.evaluate(model)\n",
    "        logger.log('[cosine   ][global] R@1: %.4f | R@5: %.4f | R@10: %.4f' % (acc['top-1'], acc['top-5'], acc['top-10']))\n",
    "        scheduler.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save({\n",
    "#    \"model\": model, \n",
    "#    \"id_cls\": classifier,\n",
    "#}, \"resnet18_bigru_512_id_initalized.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b196d3857ac2497e8508feb059522aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='train-stage-2, epoch0', max=1, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'id_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-563298ccf605>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_mlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcap_mlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriplet_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train-stage-2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mimg_mlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcap_mlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-563298ccf605>\u001b[0m in \u001b[0;36mtrain_epoch_global\u001b[0;34m(train_data, model, img_mlp_rga, cap_mlp_rga, optimizer, triplet_loss, logger, note)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# id_loss = cls_loss(classifier(img), pid.cuda()) +  cls_loss(classifier(cap), pid.cuda())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtri_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mid_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'id_loss' is not defined"
     ]
    }
   ],
   "source": [
    "def train_epoch_global(train_data, model, img_mlp_rga, cap_mlp_rga, optimizer, triplet_loss, logger, note=\"train\"):\n",
    "    model.train()\n",
    "    cum_tri_loss, cum_id_loss = 0.0, 0.0\n",
    "    for i, data in tqdm(enumerate(train_data), \"%s, epoch%d\" % (note,epoch)):\n",
    "        # load data\n",
    "        (img,pos_img,neg_img, cap, pos_cap, neg_cap, pid, pos_pid, neg_pid) = data\n",
    "        img, pos_img, neg_img = model(img.cuda()), model(pos_img.cuda()), model(neg_img.cuda())\n",
    "        cap, pos_cap, neg_cap = model(cap.cuda()), model(pos_cap.cuda()), model(neg_cap.cuda())\n",
    "        \n",
    "        # loss\n",
    "        tri_loss =  crossmodal_triplet_loss(img,pos_img,neg_img, \n",
    "                                              cap, pos_cap, neg_cap, \n",
    "                                              triplet_loss, cfg.dist_fn_opt) \n",
    "        # id_loss = cls_loss(classifier(img), pid.cuda()) +  cls_loss(classifier(cap), pid.cuda())\n",
    "        \n",
    "        loss = tri_loss + id_loss\n",
    "        \n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        \n",
    "        # log\n",
    "        cum_tri_loss += tri_loss.item()\n",
    "        if (i+1) % 64 == 0:\n",
    "            logger.log(\"batch %d, [tri-loss] %.6f\" % (i, cum_tri_loss/64))\n",
    "            cum_tri_loss = 0.0\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def regional_alignment_text(fulls, parts, p2fs, dist_fn_opt):\n",
    "    start_index = 0\n",
    "    aligned = []\n",
    "    for i, jump in enumerate(p2fs):\n",
    "        curr_parts = parts[start_index:start_index + jump]\n",
    "        start_index += jump\n",
    "        curr_full = fulls[i:i+1]\n",
    "        aligned.append(RGA_attend_one_to_many(curr_full, curr_parts, dist_fn_opt))\n",
    "    return torch.cat(aligned)\n",
    "\n",
    "def regional_alignment_image(fulls, parts, dist_fn_opt):\n",
    "    return RGA_attend_one_to_many_batch(fulls, parts, dist_fn_opt)\n",
    "  \n",
    "    \n",
    "def train_epoch_regional(train_data, model, img_mlp_rga, cap_mlp_rga, optimizer, triplet_loss, logger, note=\"train\"):\n",
    "    model.train(); img_mlp_rga.train(); cap_mlp_rga.train()\n",
    "    \n",
    "    cum_tri_loss, cum_tri_image_regional_loss, cum_tri_text_regional_loss = 0.0, 0.0, 0.0\n",
    "    for i, data in tqdm(enumerate(train_data), \"%s, epoch%d\" % (note,epoch)):\n",
    "        # load data\n",
    "        (img, pos_img, neg_img, \n",
    "         cap, pos_cap, neg_cap,\n",
    "         nps, pos_nps, neg_nps,\n",
    "         n2c, pos_n2c, neg_n2c,\n",
    "         pid, pos_pid, neg_pid) = data\n",
    "        \n",
    "        \n",
    "        img, img_part = model(img.cuda())\n",
    "        pos_img, pos_img_part = model(pos_img.cuda())\n",
    "        neg_img, neg_img_part = model(neg_img.cuda())\n",
    "        cap, pos_cap, neg_cap = model(cap.cuda()), model(pos_cap.cuda()), model(neg_cap.cuda())\n",
    "        N, M, T = nps.size()\n",
    "        nps, pos_nps, neg_nps = model(nps.cuda().reshape(-1, T)), model(pos_nps.cuda().reshape(-1, T)), model(neg_nps.cuda().reshape(-1, T))\n",
    "        \n",
    "        # part\n",
    "        img_part, pos_img_part, neg_img_part = img_mlp_rga(img_part), img_mlp_rga(pos_img_part), img_mlp_rga(neg_img_part)\n",
    "        \n",
    "        nps, pos_nps, neg_nps = cap_mlp_rga(nps.reshape(N, M, -1)), cap_mlp_rga(pos_nps.reshape(N, M, -1)), cap_mlp_rga(neg_nps.reshape(N, M, -1))\n",
    "        \n",
    "        img_part = RGA_attend_one_to_many_batch(cap, img_part, cfg.dist_fn_opt)\n",
    "        pos_img_part = RGA_attend_one_to_many_batch(pos_cap, pos_img_part, cfg.dist_fn_opt)\n",
    "        neg_img_part = RGA_attend_one_to_many_batch(neg_cap, neg_img_part, cfg.dist_fn_opt)\n",
    "        #cap_part = regional_alignment_text(img, nps, n2c, cfg.dist_fn_opt)\n",
    "        #pos_cap_part = regional_alignment_text(pos_img, pos_nps, pos_n2c, cfg.dist_fn_opt)\n",
    "        #neg_cap_part = regional_alignment_text(neg_img, neg_nps, neg_n2c, cfg.dist_fn_opt)\n",
    "        cap_part = RGA_attend_one_to_many_batch(img, nps, cfg.dist_fn_opt)\n",
    "        pos_cap_part = RGA_attend_one_to_many_batch(pos_img, pos_nps, cfg.dist_fn_opt)\n",
    "        neg_cap_part = RGA_attend_one_to_many_batch(neg_img, neg_nps, cfg.dist_fn_opt)\n",
    "        \n",
    "        # loss\n",
    "        tri_loss =  crossmodal_triplet_loss(img,pos_img,neg_img, \n",
    "                                              cap, pos_cap, neg_cap, \n",
    "                                              triplet_loss, cfg.dist_fn_opt) \n",
    "        tri_image_regional_loss =  crossmodal_triplet_loss(img_part,pos_img_part,neg_img_part, \n",
    "                                              cap, pos_cap, neg_cap, \n",
    "                                              triplet_loss, cfg.dist_fn_opt) \n",
    "        tri_text_regional_loss =  crossmodal_triplet_loss(img,pos_img,neg_img, \n",
    "                                              cap_part, pos_cap_part, neg_cap_part, \n",
    "                                              triplet_loss, cfg.dist_fn_opt) \n",
    "        \n",
    "        \n",
    "        loss = tri_loss + tri_image_regional_loss  + tri_text_regional_loss\n",
    "        \n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        \n",
    "        # log\n",
    "        cum_tri_loss += tri_loss.item()\n",
    "        cum_tri_image_regional_loss += tri_image_regional_loss.item()\n",
    "        cum_tri_text_regional_loss += tri_text_regional_loss.item()\n",
    "        \n",
    "        if (i+1) % 64 == 0:\n",
    "            logger.log(\"batch %d, [tri-loss] %.6f, [img_rga] %.6f, [cap_rga] %.6f\" % (i, \n",
    "                                                                                      cum_tri_loss/64, \n",
    "                                                                                     cum_tri_image_regional_loss / 64, \n",
    "                                                                                     cum_tri_text_regional_loss / 64))\n",
    "            cum_tri_loss, cum_tri_image_regional_loss, cum_tri_text_regional_loss = 0.0, 0.0, 0.0\n",
    "    return model\n",
    "\n",
    "train_epoch = train_epoch_regional if cfg.np else train_epoch_global\n",
    "\n",
    "# stage 1 - image channel forzen\n",
    "model.img_backbone.melt_layer(8)\n",
    "param_to_optimize = build_graph_optimizer([model, img_mlp, cap_mlp])\n",
    "optimizer = optim.Adam(param_to_optimize, lr=2e-4, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10)\n",
    "for epoch in range(0):\n",
    "    model = train_epoch(train_loader, model, img_mlp, cap_mlp, optimizer, triplet_loss, logger, \"train-stage-1\")\n",
    "    if cfg.np:\n",
    "        acc = evaluator.evaluate(model,  img_mlp, cap_mlp)\n",
    "    else:\n",
    "        acc = evaluator.evaluate(model)\n",
    "    logger.log('[euclidean][global] R@1: %.4f | R@5: %.4f | R@10: %.4f' % (acc['top-1'], acc['top-5'], acc['top-10']))\n",
    "    if cfg.np:\n",
    "        acc = cos_evaluator.evaluate(model,  img_mlp, cap_mlp)\n",
    "    else:\n",
    "        acc = cos_evaluator.evaluate(model)\n",
    "    logger.log('[cosine   ][global] R@1: %.4f | R@5: %.4f | R@10: %.4f' % (acc['top-1'], acc['top-5'], acc['top-10']))\n",
    "    scheduler.step()\n",
    "    \n",
    "\n",
    "    \n",
    "# stage 2 - train all\n",
    "model.img_backbone.melt_layer(7)\n",
    "param_to_optimize = build_graph_optimizer([model, img_mlp, cap_mlp])\n",
    "optimizer = optim.Adam(param_to_optimize, lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10)\n",
    "for epoch in range(40):\n",
    "    model = train_epoch(train_loader, model, img_mlp, cap_mlp, optimizer, triplet_loss, logger, \"train-stage-2\")\n",
    "    if cfg.np:\n",
    "        acc = evaluator.evaluate(model,  img_mlp, cap_mlp)\n",
    "    else:\n",
    "        acc = evaluator.evaluate(model)\n",
    "    logger.log('[euclidean][global] R@1: %.4f | R@5: %.4f | R@10: %.4f' % (acc['top-1'], acc['top-5'], acc['top-10']))\n",
    "    if cfg.np:\n",
    "        acc = cos_evaluator.evaluate(model,  img_mlp, cap_mlp)\n",
    "    else:\n",
    "        acc = cos_evaluator.evaluate(model)\n",
    "    logger.log('[cosine   ][global] R@1: %.4f | R@5: %.4f | R@10: %.4f' % (acc['top-1'], acc['top-5'], acc['top-10']))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage 2 - train all\n",
    "model.img_backbone.melt_layer(0)\n",
    "param_to_optimize = build_graph_optimizer([model])\n",
    "optimizer = optim.Adam(param_to_optimize, lr=2e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15)\n",
    "for epoch in range(60):\n",
    "    model = train_epoch(train_loader, model, optimizer, triplet_loss, logger, \"train-stage-2\")\n",
    "    acc = evaluator.evaluate(model)\n",
    "    logger.log('[euclidean][global] R@1: %.4f | R@5: %.4f | R@10: %.4f' % (acc['top-1'], acc['top-5'], acc['top-10']))\n",
    "    acc = cos_evaluator.evaluate(model)\n",
    "    logger.log('[cosine   ][global] R@1: %.4f | R@5: %.4f | R@10: %.4f' % (acc['top-1'], acc['top-5'], acc['top-10']))\n",
    "    \n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
